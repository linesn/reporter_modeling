{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7168a2-14b6-4479-bbcb-dcc26bbfdf09",
   "metadata": {},
   "source": [
    "# A Bayesian Model for News Reporting {-}\n",
    "\n",
    "*Nicholas Lines*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e04c8-8a5c-49b8-a296-e891edeeae63",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc05ef0d-7971-4d18-b6a1-0cea72644c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3553ecc9-a925-42d2-9cb9-6c851463f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import reporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "41fcbf53-5594-4d95-8c0c-099f3d18dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(reporter);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854d91e3-440d-4473-989a-2a3cafb9c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pymc3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print (os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329aa49f-b103-4792-bd16-3b1b865f11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import arviz as az\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "#import graphviz\n",
    "import os\n",
    "import pymc3 as pm\n",
    "from pymc3 import Model, Normal, HalfNormal, Bernoulli, Deterministic, Uniform\n",
    "from pymc3 import find_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0537ec6b-1140-47a1-af1e-a445d87c1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f655191b-4efe-4c5f-b7fa-45fc3514cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077202c5-d5de-488a-8440-8e91e788336b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# A Generative Model for News Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a62f3e-91bf-40b4-82be-d5d9dc3a721d",
   "metadata": {
    "tags": []
   },
   "source": [
    "We wish to model the information network by which a news reporter can gather facts about the news topics of interest to them. While we will use vocabulary related to the application of gathering information about news topics, we note that this task is analagous to many other information-gathering-and-fusion processes. \n",
    "\n",
    "Consider a reporter who must stay informed about $n_j$ news topics, indexed by $j$. The reporter cannot observe the facts related to these news topics directly, and relies on a network of first-hand observers to inform the reporter. This network consists of $n_i$ observers, indexed by $i$, who each provide an observation at each time step $t$ of $n_t$ timesteps. These observations, labeled $o_{i,j,t}$ (for observer $i$ speaking about topic $j$ at time $t$) are the only information provided to the reporter.\n",
    "\n",
    "Many techniques have been proposed to derive \"facts\" from text streams and other media, but we will not include these steps. Instead, we will assume that each news topic $j$ produces a single binary \"fact\" called $f_{j,t}$ at each time step $t$. (For example, the sports topic might yield facts such as \"The tigers beat the rams on Saturday,\" which can be represented as a 1 or 0 for true or false.) The fact is then observed by each observer. However, we wish to model the fact that not all observers are are equally aware of all newsworthy subjects, and not all observers will pass on their information at each time step. Therefore, we insist on the following dependencies.\n",
    "\n",
    "1. The binary fact for each topic and each timestep is sampled from a Bernoulli distribution $f_{j,t} \\sim Bern(\\delta_j)$ where $\\delta_j\\sim Unif(0,1)$ represents the bias of this news topic toward 1-valued facts. \n",
    "2. Whether an observer will report their observation or not is represented by the binary variable $\\tau_{i,j,t} \\sim Bern(\\epsilon_{i,j})$ where $\\epsilon_{i,j}\\sim Unif(0,1)$ represents that observer's bias toward reporting about this news topic.\n",
    "3. Whether an observer is well-informed about this topic's fact at time $t$ is represented by the binary variable $a_{i,j,t} \\sim Bern(\\beta_{i,j})$ where $\\beta_{i,j}\\sim Unif(0,1)$ is a hyperparameter representing how well-informed the observer is on this topic on average. This hyperparameter is the variable of most interest for us.\n",
    "4. Let $\\tilde{x}$ represent `not` $x$ for a binary variable $x$. We will use a uniform random binary variable $r_{i,j,t} \\sim Bern(0.5)$. The observer's report to the reporter is \n",
    "$$o_{i,j,t} = \\tau_{i,j,t}(a_{i,j,t}f_{j,t} + \\tilde{a_{i,j,t}}r_{i,j,t}) + 2\\tilde{\\tau_{i,j,t}},$$\n",
    "meaning that if the observer is aware and chooses to report at this timestep, they report the fact $f_{j,t}$ with no alteration; if the observer is unaware, the observer reports 1 or 0 with equal probability; and if the observer chooses not to report, a 2 is returned, signifying that no information was passed on.\n",
    "5. The reporter then constructs $$b_j = mode_i(\\{o_{i,j,t} : o_{i,j,t}\\neq2\\})$$ as an approximation to $f_{j,t}$, i.e. the reporter's best guess at the true fact.  \n",
    "\n",
    "This model is shown in the diagram below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00ffa3-5310-4cd8-98df-28f579c4a679",
   "metadata": {},
   "source": [
    "![](../Images/reporting_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd30aa-64e1-4a74-a4cf-cbe627f98f0c",
   "metadata": {},
   "source": [
    "We should make a few observations about this naive construction before continuing. \n",
    "\n",
    "1. First, the approximation $b_j \\approx f_{j,t}$ is not really legitimate because it could well be that all observers agreed to lie. Really $b_j$ tells us nothing more than the consensus of observations about the fact. This fact is important in situations where we suspect that there is not consensus or that the consensus of our observers is unreliable. \n",
    "2. The model as stated leaves each topic's network entirely independent of the other topics, so there is no real need for the outermost plate in the diagram: this was included solely to remind us of this assumption and to indicate that this problem scales in topics $j$. \n",
    "3. The purpose for laying out this generative model is to help us strategize how to learn the observer awareness hyperparameter, $\\beta_{i,j}$, which will allow us to reduce the network by cutting all edges $o_{i,j,t} - b_{j,t}$ when $\\beta_{i,j} < \\rho$ for some threshold $\\rho$.\n",
    "4. We are not interested in recovering any other hyperparameters in this situation. These exist solely to lend verisimilitude to the model.\n",
    "5. If we allow $\\lvert\\delta_j-0.5\\rvert$ to grow too small, the inference problem gets much harder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45abb29-179b-4d1b-9e2a-e9d29296a45f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Extremely Simple Inference  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c19e4-9283-4871-a62d-a473c9e3cd4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Let's consider how we could approximate $\\beta_{i,j}$ using the assumed distributions. If we condition on $\\tau_{i,j,t}=1$ we restrict ourselves to a reveiw of the informative reports from a single observer, i.e. $$\\{ o_{i,j,t}: o_{i,j,t}\\neq 2, i=\\bar{i} \\},$$\n",
    "and we can write \n",
    "$$ o_{i,j,t} = a_{i,j,t} f_{j,t} + \\tilde{a}_{i,j,t} r_{i,j,t}.$$\n",
    "\n",
    "We can quickly describe the probability distribution for $o_{i,j,t}$ under these conditions using a probability table (since these are all binary variables). We recall that the parameter of each Bernoulli random variable represents the probability that variable equals one. (For simplicity we will drop the subscripts for the table.)  \n",
    "\n",
    "\n",
    "| $o$ | $a$ | $f$ | $r$ | $P(a)$   | $P(f)$   | $P(r)$ | \n",
    "|-----|-----|-----|-----|----------|----------|--------|\n",
    "| 0   | 1   | 0   | 0   | $\\beta$  |$1-\\delta$| 0.5 (always) |\n",
    "| 1   | 1   | 1   | 0   | $\\beta$  |$\\delta$  | 0.5    |\n",
    "| 1   | 1   | 1   | 1   | $\\beta$  |$\\delta$  | 0.5    |\n",
    "| 0   | 1   | 0   | 1   | $\\beta$  |$1-\\delta$| 0.5    |\n",
    "| 1   | 0   | 1   | 1   |$1-\\beta$ |$\\delta$  | 0.5    |\n",
    "| 1   | 0   | 0   | 1   |$1-\\beta$ |$1-\\delta$| 0.5    |\n",
    "| 0   | 0   | 1   | 0   |$1-\\beta$ |$\\delta$  | 0.5    |\n",
    "| 0   | 0   | 0   | 0   |$1-\\beta$ |$1-\\delta$| 0.5    |\n",
    "\n",
    "Each of $\\beta,\\delta,$ and $r$ are mutually independent variables, so their joint probability is the product of their marginal probabilities. By summing up the joint probabilities of combinations that lead us to $o_{\\bar{i},j,t}=1$ we have\n",
    "\n",
    "\\begin{align}\n",
    "P(o_{\\bar{i},j,t}=1) &= 0.5(\\beta_{\\bar{i},j}\\delta_j + \\delta_j - \\beta_{\\bar{i},j,t}\\delta_j + 1 - \\beta_{\\bar{i},j,t} - \\delta_j + \\beta_{\\bar{i},j,t}\\delta_j + \\beta_{\\bar{i},j,t}\\delta_j)\\\\\n",
    "       &= 0.5(-\\beta_{\\bar{i},j,t} + \\beta_{\\bar{i},j,t}\\delta_j + 1)\n",
    "\\end{align}\n",
    "\n",
    "We would reach the same result with much less work using conditional probabilities (factors), since \n",
    "\\begin{align}\n",
    "P(o_{\\bar{i},j,t}=1) &= P(o_{\\bar{i},j,t}=1 \\mid a_{\\bar{i},j,t}=0) P(a_{\\bar{i},j,t}=0) + P(o_{\\bar{i},j,t}=1 \\mid a_{\\bar{i},j,t}=1) P(a_{\\bar{i},j,t}=1)\\\\\n",
    "&= 0.5(1-\\beta_{\\bar{i},j}) + \\delta_j \\beta_{\\bar{i},j}.\n",
    "\\end{align}\n",
    "\n",
    "Solving for $\\beta$ gives us\n",
    "$$\\beta_{\\bar{i},j,t} = \\frac{2P(o_{\\bar{i},j,t}=1) - 1}{2\\delta_j - 1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c439a92-d5b7-401a-beef-ffc431b4f753",
   "metadata": {},
   "source": [
    "We could approximate \n",
    "$$\\delta_j \\approx \\hat{\\delta_j} = \\frac{ \\# \\{o_{i,j,t} : o_{i,j,t}=1\\}}{ \\# \\{o_{i,j,t} : o_{i,j,t}\\neq 2 \\} },$$\n",
    "\n",
    "i.e. we use the sample mean of the informative observations to approximate the probability that a generated fact equals 1 (an extension of the approximation $b_{j,t}=\\hat{f}_{j,t}\\approx f_{j,t}$). But we already have the observed mean\n",
    "$$\\hat{\\delta_j} = \\bar{b_{j,t}},$$\n",
    "which immediately fills that role.\n",
    "\n",
    "Our statistic for $P(o_{\\bar{i},j,t}=1)$ is the observed probability\n",
    "$$ P(o_{\\bar{i},j,t}=1) = \\frac{ \\# \\{o_{\\bar{i},j,t} : o_{\\bar{i},j,t}=1\\}}{ \\# \\{o_{\\bar{i},j,t} : o_{\\bar{i},j,t}\\neq 2 \\} }.$$\n",
    "\n",
    "Now we can make the approximation\n",
    "\n",
    "\\begin{align}\n",
    "\\beta_{i,j}\\approx \\hat{\\beta_{i,j}} &= \\frac{2 \\frac{ \\# \\{o_{\\bar{i},j,t} : o_{\\bar{i},j,t}=1\\}}{ \\# \\{o_{\\bar{i},j,t} : o_{\\bar{i},j,t}\\neq 2 \\} } -1}{2\\hat{\\delta_j}-1}\\\\\n",
    "&= \\frac{2 \\frac{ \\# \\{o_{\\bar{i},j,t} : o_{\\bar{i},j,t}=1\\}}{ \\# \\{o_{\\bar{i},j,t} : o_{\\bar{i},j,t}\\neq 2 \\} } -1}{2\\frac{ \\# \\{o_{i,j,t} : o_{i,j,t}=1\\}}{ \\# \\{o_{i,j,t} : o_{i,j,t}\\neq 2 \\} } - 1}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df972774-598e-4810-9276-9779d6818d97",
   "metadata": {},
   "source": [
    "# Simulating Data with this Model\n",
    "\n",
    "We want to create an artificial dataset so we can observe this model at work, and hopefully show how to recover the desired hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a87122-c681-45e0-aa5a-825ccf1795eb",
   "metadata": {},
   "source": [
    "Generate sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d4cfa91-7439-4933-ba0b-79bde426cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_i = 20\n",
    "n_t = 1000\n",
    "data = reporter.generate_data(n_i, n_t, r_p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02629c-c459-4f4f-b956-d25b5321b634",
   "metadata": {},
   "source": [
    "## Approximate Parameter Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cdfffe4c-2bc4-4031-95cf-7abfb3686df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimators are delta_hat=0.191, b, beta_hat.\n",
      "The norm distance between epsilon_hat and epsilon is 0.04924366498246216.\n",
      "The approximation b for f is 98.4% accurate.\n",
      "The approximation delta_hat for delta=0.1860560602303919 has relative error 0.02657231247123078.\n",
      "The norm distance between beta_hat and beta is 0.5886258911574194.\n",
      "The beta_hat estimator recognizes betas less than 0.5\n",
      "90.0% of the time.\n"
     ]
    }
   ],
   "source": [
    "estimators = reporter.evaluate_estimators_full(data=data, agreement_thresh=0.5, printing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ecfbefcc-61dd-4bfe-ae2e-d77dfd263de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(o, delta, f, beta, alpha, epsilon,tau,r) = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a306b383-442d-445f-a017-f6d7875630e0",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "The next steps for this project are as follows.\n",
    "\n",
    "1. Demonstrate how to prune reporting edges from uninformed observers.  \n",
    "\n",
    "    a. Prune by convergence of $\\hat{\\beta}$  \n",
    "    \n",
    "    b. Prune by thresholding (some minimum number of timesteps)  \n",
    "    \n",
    "2. Show that this results in approximations of the fact variable comparable to the unpruned version.\n",
    "\n",
    "## Pseudocode for the Remaining Work\n",
    "\n",
    "We'll now describe the same steps using pseudocode.\n",
    "\n",
    "```Python\n",
    "set threshold_for_min_iterations\n",
    "set threshold_awareness\n",
    "set threshold_convergence\n",
    "for delta in deltas:\n",
    "    set iterations = 0\n",
    "    set c_pruned_observers = list()\n",
    "    set t_pruned_observers = list()\n",
    "    set beta_hats = dict()\n",
    "    draw epsilons\n",
    "    draw betas\n",
    "    for timestep in timesteps:\n",
    "        iterations += 1\n",
    "        draw fact\n",
    "        for observer in observers:\n",
    "            draw a\n",
    "            draw tau\n",
    "            draw r\n",
    "            form o\n",
    "            form beta_hat\n",
    "            update beta_hats[observer]\n",
    "        create b\n",
    "        create t_pruned_b\n",
    "        create c_pruned_b\n",
    "        for observer in observers:\n",
    "            if |beta_hat[observer][-1]-beta_hat[observer][-2]| < threshold_convergence:\n",
    "                c_pruned_observers.append(observer)\n",
    "            if iterations > threshold_for_min_iterations:\n",
    "                if beta_hat[observer][-1] < threshold_awareness:\n",
    "                    t_pruned_observers.append(observer)\n",
    "        \n",
    "        compare b, c_pruned_b and t_pruned_b to f\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1005de93-7d18-462b-81ac-d5341e3e5c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b8594b77204c30958776928cad87f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_ts = [10, 100, 500, 1000, 5000, 10000]\n",
    "n_j = 1 # One news topic\n",
    "n_i = 20 # observers\n",
    "n_t = 1000 # timesteps\n",
    "n_d = 10 # number of deltas to try\n",
    "average_norms = []\n",
    "average_agreements = []\n",
    "average_accuracies = []\n",
    "for n_t in tqdm(n_ts):\n",
    "#for n_i in tqdm([10,20,50]):\n",
    "    norms = []\n",
    "    agreements = []\n",
    "    accuracies = []\n",
    "    for d in range(n_d):\n",
    "        delta = st.uniform.rvs(0,1,n_j)\n",
    "        epsilon = st.uniform.rvs(0,1,n_i)\n",
    "        beta = st.uniform.rvs(0,1,n_i)\n",
    "        f = st.bernoulli(delta[0]).rvs(n_t)\n",
    "        tau = array([st.bernoulli(epsilon_ij).rvs(n_t) for epsilon_ij in epsilon])\n",
    "        a = array([st.bernoulli(beta_ij).rvs(n_t) for beta_ij in beta])\n",
    "        r = array([st.bernoulli(0.5).rvs(n_t) for i in range(n_i)])\n",
    "        o = array([[tau[i][t] * (a[i][t] * f[t] + bin_not(a[i][t])*r[i][t]) + 2 * bin_not(tau[i][t]) for i in range(n_i)] for t in range(n_t)])\n",
    "        #o = array([[1 * (a[i][t] * f[t] + bin_not(a[i][t])*r[i][t]) + 0 * bin_not(tau[i][t]) for i in range(n_i)] for t in range(n_t)])\n",
    "        # We need to drop any timestep where we got absolutely no information.\n",
    "        drop_rows = argwhere(np.all(o == 2, axis=1)).flatten()\n",
    "        o = delete(o,drop_rows,0)\n",
    "        n_t = o.shape[0]\n",
    "        b = array([st.mode([oit for oit in o[t] if oit!=2])[0][0] for t in range(n_t)])\n",
    "        f = delete(f,drop_rows)\n",
    "        accuracies.append(1-(abs(b-f).sum()/b.shape[0]))\n",
    "        #print(f\"The approximation b for f is {100*(1-(abs(b-f).sum()/b.shape[0]))}% accurate\")\n",
    "        p_o_i_bar_1 = [(o[:,i]==1).sum()/((o[:,i]!=2).sum()) for i in range(n_i)] \n",
    "        #delta_hat_j = (o==1).sum()/((o!=2).sum()) \n",
    "        delta_hat_j = mean(b)\n",
    "        beta_hat_j = array([(2 * p_i - 1) / (2*delta_hat_j - 1) for p_i in p_o_i_bar_1])\n",
    "        #beta_hat_j = array([(2 * p_i - 1) / (delta[0] - 1) for p_i in p_o_i_bar_1])\n",
    "        norms.append(norm(beta_hat_j - beta))\n",
    "        agreements.append(proportionate_threshold_agreement(beta_hat_j, beta, 0.5))\n",
    "    average_norms.append(mean(norms))\n",
    "    average_agreements.append(mean(agreements))\n",
    "    average_accuracies.append(mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a3ba6-6596-41f9-be3c-191dc2168c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
